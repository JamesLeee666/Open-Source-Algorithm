{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd731bbf-fd05-47f3-b67e-144950061b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-01158ae5cc48>:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__\n",
    "tf.__path__\n",
    "tf.test.is_gpu_available()\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e69612-085d-44e3-97b2-362d7aafbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "# import preprocess\n",
    "# from preprocess import gen_data_set, gen_model_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss\n",
    "\n",
    "import random\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.compiler import compiles\n",
    "from sqlalchemy.sql.expression import Insert\n",
    "\n",
    "import traceback;\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.min_rows', 50)\n",
    "pd.set_option('display.max_columns', None) #显示所有列\n",
    "# pd.set_option('display.max_rows', None) #显示所有行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29cfd0d-e73d-4918-864b-e6ae1a0cd03d",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c209972e-b0f1-4c28-9137-cce826e35262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget http://files.grouplens.org/datasets/movielens/ml-1m.zip -O ./ml-1m.zip \n",
    "# ! wget https://raw.githubusercontent.com/shenweichen/DeepMatch/master/examples/preprocess.py -O preprocess.py\n",
    "# ! unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5caa9a30-71b3-4658-aa94-6a3de1dc4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6db5c05a1769>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
      "<ipython-input-3-6db5c05a1769>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
      "<ipython-input-3-6db5c05a1769>:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp  \\\n",
       "0        1      1193       5  978300760   \n",
       "1        1       661       3  978302109   \n",
       "2        1       914       3  978301968   \n",
       "3        1      3408       4  978300275   \n",
       "4        1      2355       5  978824291   \n",
       "\n",
       "                                    title                        genres  \\\n",
       "0  One Flew Over the Cuckoo's Nest (1975)                         Drama   \n",
       "1        James and the Giant Peach (1996)  Animation|Children's|Musical   \n",
       "2                     My Fair Lady (1964)               Musical|Romance   \n",
       "3                  Erin Brockovich (2000)                         Drama   \n",
       "4                    Bug's Life, A (1998)   Animation|Children's|Comedy   \n",
       "\n",
       "  gender  age  occupation    zip  \n",
       "0      F    1          10  48067  \n",
       "1      F    1          10  48067  \n",
       "2      F    1          10  48067  \n",
       "3      F    1          10  48067  \n",
       "4      F    1          10  48067  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./\"\n",
    "\n",
    "# 用户ID，性别，年龄，职业，zip（外加：地理位置如国家城市）\n",
    "unames = ['user_id','gender','age','occupation','zip']\n",
    "user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
    "\n",
    "# 用户ID，电影ID，评分，时间\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
    "\n",
    "# 电影ID，电影名，类型\n",
    "mnames = ['movie_id','title','genres']\n",
    "movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames)\n",
    "\n",
    "# 将三个数据表拼成一个表\n",
    "data = pd.merge(pd.merge(ratings,movies),user)#.iloc[:5000]\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e21cdc8f-bef5-4e22-93ec-790faab5655f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用户数量和电影数量\n",
    "len(data['user_id'].unique()),len(data['movie_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d09182b-a381-4a1a-9330-ae02dcdbcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_set(data, negsample=1):\n",
    "    # 按照时间戳进行升序排序\n",
    "    data.sort_values(\"timestamp\", inplace=True)\n",
    "    # 获取唯一的电影id\n",
    "    item_ids = data['movie_id'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')):\n",
    "        # 获取当前用户看过的电影ID\n",
    "        pos_list = hist['movie_id'].tolist()\n",
    "        # 获取当前用户看过的电影的评分\n",
    "        rating_list = hist['rating'].tolist()\n",
    "        \n",
    "        \n",
    "        if len(pos_list) == 1:\n",
    "            train_set.append((reviewerID, [pos_list[0]], pos_list[0], 1,1,rating_list[0]))\n",
    "        # 如果负样本数大于0\n",
    "        if negsample > 0:\n",
    "            # 获取未被看过的电影ID\n",
    "            # 可以将候选集生成的过程看成是一个极端的多分类问题。那么推荐问题就转化成了一个预测分类准确性的问题。\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            # 随机获取负采样列表\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)\n",
    "        \n",
    "        # 当 len(pos_list)>=2时才会有值, 且数字为从1开始的整数, 也就是只有当用户看过两部以上的电影时, 才会被放入训练集\n",
    "        for i in range(1, len(pos_list)):\n",
    "            # 获取除去最后一次看过的电影ID序列\n",
    "            hist = pos_list[:i]\n",
    "            # 当数组长度不是最后一个电影数据时\n",
    "            if i != len(pos_list) - 1:\n",
    "                # 将数据分割为 除去最后一次历史看过的电影ID序列, 最后一次看过的电影ID，[::-1]含义是将数组倒叙排列, 设置正样本值为1\n",
    "                # hist[::-1]为 用户观看的movie序列特征，根据观看的时间倒排，即最新观看的movieID排在前面\n",
    "                # len(hist[::-1]) 为 用户观看的movie序列长度特征，连续特征；\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1]),rating_list[i]))\n",
    "                \n",
    "                # 使用没有评分记录的数据, 生成少量负样本, 放入训练集合\n",
    "                for negi in range(negsample):\n",
    "                    #[::-1]含义是将数组倒叙排列, 设置负样本值为0\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1])))\n",
    "            else:\n",
    "                # 将最长的那一个序列长度作为测试数据\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i],1,len(hist[::-1]),rating_list[i]))\n",
    "    # 随机洗牌，打乱顺序\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print(len(train_set[0]),len(test_set[0]))\n",
    "\n",
    "    return train_set,test_set\n",
    "\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "    \n",
    "    # 用0填补缺失值\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    # 标签 movie_id: 最后一次的点击视频id号\n",
    "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\"hist_len\": train_hist_len}\n",
    "    # 添加用户信息\n",
    "    for key in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee75967-8e35-404d-b48b-9f0fa48e633d",
   "metadata": {},
   "source": [
    "# 构建特征列，训练模型，导出embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3315bb-40cb-441f-99a4-858ac01dfbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:25<00:00, 232.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6\n",
      "WARNING:tensorflow:From /opt/miniconda39/envs/tf2/lib/python3.9/site-packages/tensorflow/python/keras/initializers/initializers_v1.py:47: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda39/envs/tf2/lib/python3.9/site-packages/tensorflow/python/keras/initializers/initializers_v1.py:47: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\", ]\n",
    "SEQ_LEN = 50\n",
    "negsample = 1\n",
    "\n",
    "# 1. 首先对于数据中的特征进行ID化编码，然后使用 `gen_date_set` and `gen_model_input`来生成带有用户历史行为序列的特征数据\n",
    "\n",
    "\n",
    "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip']\n",
    "feature_max_idx = {}\n",
    "for feature in features:\n",
    "    lbe = LabelEncoder()\n",
    "    # 将每一列数据标签化并+1\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    # 获取标签化后每列数据的最大值+1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "    \n",
    "# 构建用户画像\n",
    "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
    "# 构建物品画像\n",
    "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "# 用户历史点击文章序列\n",
    "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
    "\n",
    "train_set, test_set = gen_data_set(data, negsample)\n",
    "\n",
    "\n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
    "\n",
    "# 2.计算每个稀疏字段的独特特征并为序列特征生成特征配置\n",
    "\n",
    "# 配置一下模型定义需要的特征列，主要是特征名和embedding词表的大小\n",
    "embedding_dim = 32\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce43115b-1cf8-4b2d-a12d-3ef8cc1b83b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "算法开始执行:2021-09-27 15:47:52.915670\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7fbdfc283370>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7fbdfc283370>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method SequencePoolingLayer.call of <deepctr.layers.sequence.SequencePoolingLayer object at 0x7fbdfc283370>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fbdfc241880>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fbdfc241880>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method DNN.call of <deepctr.layers.core.DNN object at 0x7fbdfc241880>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x7fbdfc1e0e80>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x7fbdfc1e0e80>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method PoolingLayer.call of <deepmatch.layers.core.PoolingLayer object at 0x7fbdfc1e0e80>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Train on 1581006 samples, validate on 395252 samples\n",
      "Epoch 1/20\n",
      "1564672/1581006 [============================>.] - ETA: 0s - loss: 3.8729"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda39/envs/tf2/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1581006/1581006 [==============================] - 6s 4us/sample - loss: 3.8737 - val_loss: 3.8670\n",
      "Epoch 2/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.8491 - val_loss: 3.8747\n",
      "Epoch 3/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.8402 - val_loss: 3.8702\n",
      "Epoch 4/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.8368 - val_loss: 3.8295\n",
      "Epoch 5/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.8352 - val_loss: 3.8378\n",
      "Epoch 6/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.8346 - val_loss: 3.8018\n",
      "Epoch 7/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7959 - val_loss: 3.7734\n",
      "Epoch 8/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.8011 - val_loss: 3.7928\n",
      "Epoch 9/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7750 - val_loss: 3.7726\n",
      "Epoch 10/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7955 - val_loss: 3.7789\n",
      "Epoch 11/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7734 - val_loss: 3.7622\n",
      "Epoch 12/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7531 - val_loss: 3.7629\n",
      "Epoch 13/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7625 - val_loss: 3.7694\n",
      "Epoch 14/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7540 - val_loss: 3.7327\n",
      "Epoch 15/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7125 - val_loss: 3.7577\n",
      "Epoch 16/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7126 - val_loss: 3.6946\n",
      "Epoch 17/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7106 - val_loss: 3.7169\n",
      "Epoch 18/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.7179 - val_loss: 3.7416\n",
      "Epoch 19/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.6796 - val_loss: 3.7204\n",
      "Epoch 20/20\n",
      "1581006/1581006 [==============================] - 4s 3us/sample - loss: 3.6615 - val_loss: 3.6778\n",
      "(6040, 32)\n",
      "(3706, 32)\n",
      "算法执行完成:2021-09-27 15:49:17.263470\n",
      "总耗时: 0:01:24.347800\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(f\"算法开始执行:{start_time}\")\n",
    "\n",
    "# 3.搭建模型并训练\n",
    "\n",
    "# 定义一个YoutubeDNN模型，分别传入用户侧特征列表user_feature_columns和物品侧特征列表item_feature_columns。然后配置优化器和损失函数，开始进行训练。\n",
    "\n",
    "K.set_learning_phase(True)\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=100, user_dnn_hidden_units=(128,64, embedding_dim))\n",
    "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=False,p=1,k_max=2,num_sampled=100,user_dnn_hidden_units=(128,64, embedding_dim))\n",
    "\n",
    "# 配置优化器和损失函数\n",
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\n",
    "\n",
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=8192, epochs=20, verbose=1, validation_split=0.2, )\n",
    "\n",
    "# 4. 生成用于测试的用户特征和用于检索的完整项目特征\n",
    "train_user_model_input = train_model_input####\n",
    "\n",
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
    "\n",
    "# 以下两行是deepmatch中的通用使用方法，分别获得用户向量模型和物品向量模型\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "\n",
    "\n",
    "# 输入对应的数据拿到对应的向量\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_dict = {\"device_ids\": train_user_model_input.get(\"device_id\"), \"user_embs\": user_embs}####\n",
    "\n",
    "# user_embs = user_embs[:, i, :]  # i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "# item_dict = {\"label_id\": test_user_model_input.get(\"label_id\"), \"item_embs\": item_embs}####\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "time_cost = end_time - start_time\n",
    "print(f\"算法执行完成:{end_time}\\n总耗时: {time_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ea8a0-9d5e-460b-a4df-a7a9d6c7642b",
   "metadata": {},
   "source": [
    "# 使用faiss进行ANN查找并评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80cd3e3f-d013-42b7-a3ee-36cdc4f004b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6040it [00:01, 4412.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall：   0.11937086092715232\n",
      "hit-rate： 0.11937086092715232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from deepmatch.utils import recall_N\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)#32\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)#3706*32\n",
    "# faiss.normalize_L2(user_embs)6040*32\n",
    "\n",
    "\n",
    "# D:最近邻居的距离，shape (n, k),当没有找到足够多的结果时标签设置为 +Inf 或 -Inf\n",
    "# I:最近邻居的标签，shape (n, k),当没有找到足够的结果时，标签设置为 -1\n",
    "D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
    "s = []\n",
    "hit = 0\n",
    "\n",
    "# enumerate获取用户ID及对应的索引\n",
    "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
    "    try:\n",
    "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
    "        filter_item = None\n",
    "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
    "        s.append(recall_score)\n",
    "        if test_true_label[uid] in pred:\n",
    "            hit += 1\n",
    "    except:\n",
    "        print(i)\n",
    "print(\"\")\n",
    "# 将多次抽样的计算的结果的平均值当作最终的recall\n",
    "print(\"recall：  \", np.mean(s))\n",
    "print(\"hit-rate：\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7337198d-e631-438b-a5e3-705184eece8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_test(test_set,embedding_dim,item_embs,user_embs,test_user_model_input,item_profile):    \n",
    "    test_true_label = {line[0]:[line[2]] for line in test_set}\n",
    "\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    from tqdm import tqdm\n",
    "    from deepmatch.utils import recall_N\n",
    "\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    # faiss.normalize_L2(item_embs)\n",
    "    index.add(item_embs)\n",
    "    # faiss.normalize_L2(user_embs)\n",
    "\n",
    "\n",
    "    # D:最近邻居的距离，shape (n, k),当没有找到足够多的结果时标签设置为 +Inf 或 -Inf\n",
    "    # I:最近邻居的标签，shape (n, k),当没有找到足够的结果时，标签设置为 -1\n",
    "    D, I = index.search(np.ascontiguousarray(user_embs), 50)\n",
    "    s = []\n",
    "    hit = 0\n",
    "\n",
    "    # enumerate获取用户ID及对应的索引\n",
    "    for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
    "        try:\n",
    "            pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
    "            filter_item = None\n",
    "            recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
    "            s.append(recall_score)\n",
    "            if test_true_label[uid] in pred:\n",
    "                hit += 1\n",
    "        except:\n",
    "            print(i)\n",
    "    print(\"\")\n",
    "    # 将多次抽样的计算的结果的平均值当作最终的recall\n",
    "    print(\"recall：  \", np.mean(s))\n",
    "    print(\"hit-rate：\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32404b04-8244-4a1c-8cee-8eb487df8d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6040it [00:01, 4409.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall：   0.10264900662251655\n",
      "hit-rate： 0.10264900662251655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall_test(test_set,embedding_dim,item_embs,user_embs,test_user_model_input,item_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a80da350-fa0d-4107-8b0a-4a816014e8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "import pydot_ng as pydot\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"TB\", to_file=\"./imgs/model.png\")\n",
    "\n",
    "plot_model(user_embedding_model, show_shapes=True, show_layer_names=True, rankdir=\"TB\", to_file=\"./imgs/dnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea1bf7-2911-472d-a382-6a8c024333f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3eb021c2-a396-4e70-a15c-c6a41adb4c1a",
   "metadata": {},
   "source": [
    "# 模型保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d906e5-b67c-430c-97af-35ba7603f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要保存/加载权重，您可以像任何其他 keras 模型一样编写代码。\n",
    "model = YoutubeDNN()\n",
    "model.save_weights('YoutubeDNN_w.h5')\n",
    "model.load_weights('YoutubeDNN_w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5be6a-9d15-4a8a-8e9c-17211adda7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存/加载模型，只是有点不同。\n",
    "\n",
    "from tensorflow.python.keras.models import  save_model,load_model\n",
    "model = DeepFM()\n",
    "save_model(model, 'YoutubeDNN.h5')# save_model, same as before\n",
    "\n",
    "from deepmatch.layers import custom_objects\n",
    "model = load_model('YoutubeDNN.h5',custom_objects)# load_model,just add a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717d8c2-0b70-4dc8-8fee-9ead5d09a840",
   "metadata": {},
   "source": [
    "# 设置学习率并使用earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267e707-b12a-4a90-a35d-c3fd371b4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#您可以在 DeepCTR 中使用任何模型，例如 keras 模型对象。以下是如何设置学习率和提前停止的示例：\n",
    "\n",
    "import deepmatch\n",
    "from tensorflow.python.keras.optimizers import Adam,Adagrad\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = deepmatch.models.FM(user_feature_columns,item_feature_columns)\n",
    "model.compile(Adagrad(0.01),'binary_crossentropy',metrics=['binary_crossentropy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_binary_crossentropy')\n",
    "history = model.fit(model_input, data[target].values,batch_size=256, epochs=10, verbose=2, validation_split="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9b258-bb41-49bc-ac31-fff61409464f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35071753-6794-411a-9b10-2649c0be927c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e1007-52f7-46bc-88f9-28812ccade2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c18ca-0bf3-4d9d-a5e5-f5ccd1bb4d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
